{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy[transformers]\n",
    "python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Barack Obama\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " was born in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Hawaii\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and served as the \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    44th\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " President of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Load spaCy's model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Text to analyze\n",
    "text = \"Barack Obama was born in Hawaii and served as the 44th President of the United States.\"\n",
    "\n",
    "# Process and render\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walks      ➜ Lemma: walk       | Tense: ['Pres'] | Mood: [] | Person: ['3'] | Number: ['Sing']\n",
      "ate        ➜ Lemma: eat        | Tense: ['Past'] | Mood: [] | Person: [] | Number: []\n",
      "runs       ➜ Lemma: run        | Tense: ['Pres'] | Mood: [] | Person: ['3'] | Number: ['Sing']\n"
     ]
    }
   ],
   "source": [
    "# Input text\n",
    "text = \"He walks to school and ate lunch before he runs again.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate through tokens and show conjugation info for verbs\n",
    "for token in doc:\n",
    "    if token.pos_ == \"VERB\" or token.tag_.startswith(\"V\"):\n",
    "        print(f\"{token.text:10} ➜ Lemma: {token.lemma_:10} | Tense: {token.morph.get('Tense')} | Mood: {token.morph.get('Mood')} | Person: {token.morph.get('Person')} | Number: {token.morph.get('Number')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Independent Clauses with Tense:\n",
      "\n",
      "Clause: She walks to school , but he stayed home because it was raining .\n",
      "  Verb: walks      | Tense: ['Pres'] | Mood: [] | Aspect: []\n",
      "  Verb: stayed     | Tense: ['Past'] | Mood: [] | Aspect: []\n"
     ]
    }
   ],
   "source": [
    "text = \"She walks to school, but he stayed home because it was raining.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Helper to get tense info from a verb token\n",
    "def get_verb_tense(token):\n",
    "    return {\n",
    "        'text': token.text,\n",
    "        'lemma': token.lemma_,\n",
    "        'tense': token.morph.get(\"Tense\"),\n",
    "        'mood': token.morph.get(\"Mood\"),\n",
    "        'aspect': token.morph.get(\"Aspect\"),\n",
    "        'person': token.morph.get(\"Person\"),\n",
    "        'number': token.morph.get(\"Number\")\n",
    "    }\n",
    "\n",
    "# Identify root verbs of each independent clause\n",
    "print(\"Detected Independent Clauses with Tense:\")\n",
    "for sent in doc.sents:\n",
    "    # Normally one ROOT per clause, but can have conjuncts\n",
    "    root = [token for token in sent if token.dep_ == \"ROOT\"]\n",
    "    clause_tenses = []\n",
    "\n",
    "    for verb in root:\n",
    "        # Add conj/compound verbs as well\n",
    "        related_verbs = [verb] + [child for child in verb.children if child.dep_ in {\"conj\", \"xcomp\"}]\n",
    "        for v in related_verbs:\n",
    "            if v.pos_ == \"VERB\" or v.tag_.startswith(\"V\"):\n",
    "                clause_tenses.append(get_verb_tense(v))\n",
    "\n",
    "    print(f\"\\nClause: {' '.join([token.text for token in sent])}\")\n",
    "    for tense_info in clause_tenses:\n",
    "        print(f\"  Verb: {tense_info['text']:<10} | Tense: {tense_info['tense']} | Mood: {tense_info['mood']} | Aspect: {tense_info['aspect']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Independent Clauses with Tense, Mood, and Aspect:\n",
      "\n",
      "Clause: She walks to school , but he stayed home because it was raining .\n",
      "  Verb: walks      | Tense: Pres | Mood: None | Aspect: None\n",
      "  Verb: stayed     | Tense: Past | Mood: None | Aspect: None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample text input\n",
    "text = \"She walks to school, but he stayed home because it was raining.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Function to extract morphological features from a verb token\n",
    "def get_morph_features(token):\n",
    "    morph = token.morph.to_dict()\n",
    "    return {\n",
    "        'text': token.text,\n",
    "        'lemma': token.lemma_,\n",
    "        'tense': morph.get('Tense'),\n",
    "        'mood': morph.get('Mood'),\n",
    "        'aspect': morph.get('Aspect'),\n",
    "        'person': morph.get('Person'),\n",
    "        'number': morph.get('Number'),\n",
    "    }\n",
    "\n",
    "# Function to determine if a verb is part of an independent clause\n",
    "def is_independent_verb(token):\n",
    "    return token.dep_ in {\"ROOT\", \"conj\"} and token.head.dep_ not in {\"advcl\", \"ccomp\", \"acl\", \"relcl\", \"xcomp\"}\n",
    "\n",
    "# Iterate through sentences and extract morphological features of independent verbs\n",
    "print(\"Detected Independent Clauses with Tense, Mood, and Aspect:\")\n",
    "for sent in doc.sents:\n",
    "    clause_text = \" \".join([token.text for token in sent])\n",
    "    independent_verbs = [token for token in sent if is_independent_verb(token)]\n",
    "    clause_features = []\n",
    "\n",
    "    for verb in independent_verbs:\n",
    "        if verb.pos_ == \"VERB\":\n",
    "            features = get_morph_features(verb)\n",
    "            clause_features.append(features)\n",
    "\n",
    "    print(f\"\\nClause: {clause_text}\")\n",
    "    for features in clause_features:\n",
    "        print(f\"  Verb: {features['text']:<10} | Tense: {features['tense']} | Mood: {features['mood']} | Aspect: {features['aspect']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harpe\\Miniconda3\\Lib\\site-packages\\thinc\\shims\\pytorch.py:261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(filelike, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "── Raw morph features ──\n",
      "She        → {'Case': 'Nom', 'Gender': 'Fem', 'Number': 'Sing', 'Person': '3', 'PronType': 'Prs'}\n",
      "walks      → {'Number': 'Sing', 'Person': '3', 'Tense': 'Pres', 'VerbForm': 'Fin'}\n",
      "to         → {}\n",
      "school     → {'Number': 'Sing'}\n",
      ",          → {'PunctType': 'Comm'}\n",
      "but        → {'ConjType': 'Cmp'}\n",
      "he         → {'Case': 'Nom', 'Gender': 'Masc', 'Number': 'Sing', 'Person': '3', 'PronType': 'Prs'}\n",
      "had        → {'Tense': 'Past', 'VerbForm': 'Fin'}\n",
      "stayed     → {'Aspect': 'Perf', 'Tense': 'Past', 'VerbForm': 'Part'}\n",
      "home       → {}\n",
      "because    → {}\n",
      "it         → {'Case': 'Nom', 'Gender': 'Neut', 'Number': 'Sing', 'Person': '3', 'PronType': 'Prs'}\n",
      "was        → {'Mood': 'Ind', 'Number': 'Sing', 'Person': '3', 'Tense': 'Past', 'VerbForm': 'Fin'}\n",
      "raining    → {'Aspect': 'Prog', 'Tense': 'Pres', 'VerbForm': 'Part'}\n",
      ".          → {'PunctType': 'Peri'}\n",
      "\n",
      "── Independent-clause verbs ──\n",
      "\n",
      "Clause: She walks to school, but he had stayed home because it was raining.\n",
      "  Verb: walks      | Tense: Pres  | Mood: ['Ind']  | Aspect: —\n",
      "  Verb: stayed     | Tense: Past  | Mood: —  | Aspect: Perf\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# 1) Load transformer model\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "text = \"She walks to school, but he had stayed home because it was raining.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# 2) Debug: print raw morphological info for each token\n",
    "print(\"── Raw morph features ──\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text:10} → {token.morph.to_dict()}\")\n",
    "print()\n",
    "\n",
    "def derive_morph(token):\n",
    "    m = token.morph.to_dict()\n",
    "    # start with whatever spaCy gives\n",
    "    tense  = m.get(\"Tense\")\n",
    "    mood   = m.get(\"Mood\")\n",
    "    aspect = m.get(\"Aspect\")\n",
    "\n",
    "    # fallback: if no Mood but finite verb, assume Indicative\n",
    "    if not mood and m.get(\"VerbForm\") == \"Fin\":\n",
    "        mood = [\"Ind\"]\n",
    "    # fallback: if no Aspect but gerund or past participle\n",
    "    if not aspect:\n",
    "        if token.tag_ == \"VBG\":\n",
    "            aspect = [\"Prog\"]\n",
    "        elif token.tag_ == \"VBN\":\n",
    "            aspect = [\"Perf\"]\n",
    "        else:\n",
    "            aspect = []\n",
    "\n",
    "    # fallback: if no Tense, derive from PTB tag\n",
    "    if not tense:\n",
    "        if token.tag_ in {\"VBD\", \"VBN\"}:\n",
    "            tense = [\"Past\"]\n",
    "        elif token.tag_ in {\"VBZ\", \"VBP\", \"VB\"}:\n",
    "            tense = [\"Pres\"]\n",
    "\n",
    "    return {\n",
    "        \"text\":   token.text,\n",
    "        \"lemma\":  token.lemma_,\n",
    "        \"tense\":  tense,\n",
    "        \"mood\":   mood,\n",
    "        \"aspect\": aspect\n",
    "    }\n",
    "\n",
    "def is_independent_verb(token):\n",
    "    return (\n",
    "        token.pos_ == \"VERB\"\n",
    "        and token.dep_ in {\"ROOT\", \"conj\"}\n",
    "        and token.head.dep_ not in {\"advcl\", \"ccomp\", \"acl\", \"relcl\", \"xcomp\"}\n",
    "    )\n",
    "\n",
    "# 3) Print only independent-clause verbs with their finalized features\n",
    "print(\"── Independent-clause verbs ──\")\n",
    "for sent in doc.sents:\n",
    "    clause = sent.text.strip()\n",
    "    verbs  = [t for t in sent if is_independent_verb(t)]\n",
    "    if not verbs:\n",
    "        continue\n",
    "    print(f\"\\nClause: {clause}\")\n",
    "    for v in verbs:\n",
    "        f = derive_morph(v)\n",
    "        print(\n",
    "            f\"  Verb: {f['text']:<10} \"\n",
    "            f\"| Tense: {f['tense'] or '—'}  \"\n",
    "            f\"| Mood: {f['mood'] or '—'}  \"\n",
    "            f\"| Aspect: {f['aspect'] or '—'}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID  Event Phrase                                  Tense\n",
      "-----------------------------------------------------------------\n",
      "1   She walks to school, but he had stayed home because <EVENT 3> Pres\n",
      "2   he had stayed home because <EVENT 3>          Past\n",
      "3   it was raining                                Pres\n"
     ]
    }
   ],
   "source": [
    "text = \"She walks to school, but he had stayed home because it was raining.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Labels for clause types\n",
    "SUB_DEPS = {\"advcl\", \"ccomp\", \"acl\", \"relcl\", \"xcomp\"}  # subordinate\n",
    "CC_DEP   = \"cc\"                                         # coordinating conj\n",
    "CMP_DEP   = \"ccomp\"                                     # comparative conj\n",
    "\n",
    "def get_tense(tok):\n",
    "    t = tok.morph.get(\"Tense\")\n",
    "    return t[0] if t else \"—\"\n",
    "\n",
    "# Collect independent-clause verbs and subordinate-clause verbs\n",
    "indep = [t for t in doc if t.pos_ == \"VERB\" and t.dep_ in {\"ROOT\",\"conj\"} and t.head.dep_ not in SUB_DEPS]\n",
    "subord = []\n",
    "for root in indep:\n",
    "    for child in root.children:\n",
    "        if child.pos_ == \"VERB\" and child.dep_ in SUB_DEPS:\n",
    "            subord.append(child)\n",
    "\n",
    "# Assign IDs\n",
    "events = indep + subord\n",
    "event_id = {tok: i+1 for i, tok in enumerate(events)}\n",
    "\n",
    "def build_span(root, kind):\n",
    "    \"\"\"\n",
    "    For 'indep', include everything in subtree except:\n",
    "      - punctuation\n",
    "      - cc-subtrees\n",
    "      - cmp-subtrees (comparative conjunctions)\n",
    "      - tokens under SUB_DEPS branches (except their 'mark')\n",
    "    For 'sub', include entire subordinate subtree but drop its 'mark' and punctuation and cc-subtrees.\n",
    "    \"\"\"\n",
    "    exclude = set()\n",
    "    for child in root.children:\n",
    "        if child.dep_ == CC_DEP or child.dep_ == CMP_DEP:\n",
    "            exclude.update(child.subtree)\n",
    "\n",
    "    tokens = []\n",
    "    for t in root.subtree:\n",
    "        if t.is_punct or t in exclude:\n",
    "            continue\n",
    "        if kind == \"indep\":\n",
    "            # find which direct child of root this token descends from\n",
    "            # via ancestors\n",
    "            direct = None\n",
    "            for anc in t.ancestors:\n",
    "                if anc.head == root:\n",
    "                    direct = anc\n",
    "                    break\n",
    "            # if that direct child is a subordinate clause, skip all its tokens except the 'mark'\n",
    "            if direct and direct.dep_ in SUB_DEPS and t.dep_ != \"mark\":\n",
    "                continue\n",
    "        elif kind == \"sub\":\n",
    "            if t.dep_ == \"mark\":\n",
    "                continue\n",
    "        tokens.append(t)\n",
    "\n",
    "    if not tokens:\n",
    "        tokens = [root]\n",
    "\n",
    "    tokens = sorted(tokens, key=lambda x: x.i)\n",
    "    return doc[tokens[0].i : tokens[-1].i + 1]\n",
    "\n",
    "# Build event records\n",
    "records = []\n",
    "for tok in events:\n",
    "    kind = \"indep\" if tok in indep else \"sub\"\n",
    "    span = build_span(tok, kind)\n",
    "    records.append({\n",
    "        \"tok\": tok,\n",
    "        \"span\": span,\n",
    "        \"kind\": kind,\n",
    "        \"tense\": get_tense(tok),\n",
    "        \"id\": event_id[tok]\n",
    "    })\n",
    "\n",
    "# Sort by occurrence\n",
    "records.sort(key=lambda r: r[\"span\"].start_char)\n",
    "\n",
    "# Build subordinate lookup for replacements\n",
    "sub_lookup = {r[\"span\"]: r[\"id\"] for r in records if r[\"kind\"] == \"sub\"}\n",
    "\n",
    "def annotate_indep(span):\n",
    "    text = span.text\n",
    "    base = span.start_char\n",
    "    # find subordinate spans inside this span\n",
    "    inside = [\n",
    "        (sp, sid) for sp, sid in sub_lookup.items()\n",
    "        if sp.start_char >= base and sp.end_char <= span.end_char\n",
    "    ]\n",
    "    # replace in reverse order to keep indexes valid\n",
    "    for sp, sid in sorted(inside, key=lambda x: x[0].start_char, reverse=True):\n",
    "        s = sp.start_char - base\n",
    "        e = sp.end_char   - base\n",
    "        text = text[:s] + f\"<EVENT {sid}>\" + text[e:]\n",
    "    return text.strip()\n",
    "\n",
    "# Output\n",
    "print(f\"{'ID':<3} {'Event Phrase':<45} Tense\")\n",
    "print(\"-\"*65)\n",
    "for rec in records:\n",
    "    phrase = annotate_indep(rec[\"span\"]) if rec[\"kind\"] == \"indep\" else rec[\"span\"].text\n",
    "    print(f\"{rec['id']:<3} {phrase:<45} {rec['tense']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Independent Clauses with Tense:\n",
      "\n",
      "Clause: She walks to school , but he stayed home because it was raining .\n",
      "  Verb: walks      | Tense: ['Pres'] | Mood: [] | Aspect: []\n",
      "  Verb: stayed     | Tense: ['Past'] | Mood: [] | Aspect: []\n"
     ]
    }
   ],
   "source": [
    "text = \"She walks to school, but he stayed home because it was raining.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Helper to get tense info from a verb token\n",
    "def get_verb_tense(token):\n",
    "    return {\n",
    "        'text': token.text,\n",
    "        'lemma': token.lemma_,\n",
    "        'tense': token.morph.get(\"Tense\"),\n",
    "        'mood': token.morph.get(\"Mood\"),\n",
    "        'aspect': token.morph.get(\"Aspect\"),\n",
    "        'person': token.morph.get(\"Person\"),\n",
    "        'number': token.morph.get(\"Number\")\n",
    "    }\n",
    "\n",
    "# Identify root verbs of each independent clause\n",
    "print(\"Detected Independent Clauses with Tense:\")\n",
    "for sent in doc.sents:\n",
    "    # Normally one ROOT per clause, but can have conjuncts\n",
    "    root = [token for token in sent if token.dep_ == \"ROOT\"]\n",
    "    clause_tenses = []\n",
    "\n",
    "    for verb in root:\n",
    "        # Add conj/compound verbs as well\n",
    "        related_verbs = [verb] + [child for child in verb.children if child.dep_ in {\"conj\", \"xcomp\"}]\n",
    "        for v in related_verbs:\n",
    "            if v.pos_ == \"VERB\" or v.tag_.startswith(\"V\"):\n",
    "                clause_tenses.append(get_verb_tense(v))\n",
    "\n",
    "    print(f\"\\nClause: {' '.join([token.text for token in sent])}\")\n",
    "    for tense_info in clause_tenses:\n",
    "        print(f\"  Verb: {tense_info['text']:<10} | Tense: {tense_info['tense']} | Mood: {tense_info['mood']} | Aspect: {tense_info['aspect']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Independent Clauses with Tense:\n",
      "\n",
      "Clause: She walks to school , but he stayed home because it was raining .\n",
      "  Verb: walks      | Tense: ['Pres'] | Mood: [] | Aspect: []\n",
      "  Verb: stayed     | Tense: ['Past'] | Mood: [] | Aspect: []\n"
     ]
    }
   ],
   "source": [
    "text = \"She walks to school, but he stayed home because it was raining.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Helper to get tense info from a verb token\n",
    "def get_verb_tense(token):\n",
    "    return {\n",
    "        'text': token.text,\n",
    "        'lemma': token.lemma_,\n",
    "        'tense': token.morph.get(\"Tense\"),\n",
    "        'mood': token.morph.get(\"Mood\"),\n",
    "        'aspect': token.morph.get(\"Aspect\"),\n",
    "        'person': token.morph.get(\"Person\"),\n",
    "        'number': token.morph.get(\"Number\")\n",
    "    }\n",
    "\n",
    "# Identify root verbs of each independent clause\n",
    "print(\"Detected Independent Clauses with Tense:\")\n",
    "for sent in doc.sents:\n",
    "    # Normally one ROOT per clause, but can have conjuncts\n",
    "    root = [token for token in sent if token.dep_ == \"ROOT\"]\n",
    "    clause_tenses = []\n",
    "\n",
    "    for verb in root:\n",
    "        # Add conj/compound verbs as well\n",
    "        related_verbs = [verb] + [child for child in verb.children if child.dep_ in {\"conj\", \"xcomp\"}]\n",
    "        for v in related_verbs:\n",
    "            if v.pos_ == \"VERB\" or v.tag_.startswith(\"V\"):\n",
    "                clause_tenses.append(get_verb_tense(v))\n",
    "\n",
    "    print(f\"\\nClause: {' '.join([token.text for token in sent])}\")\n",
    "    for tense_info in clause_tenses:\n",
    "        print(f\"  Verb: {tense_info['text']:<10} | Tense: {tense_info['tense']} | Mood: {tense_info['mood']} | Aspect: {tense_info['aspect']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID  Event Phrase                                  Tense\n",
      "-----------------------------------------------------------------\n",
      "1   She walks to school, but he had stayed home because <EVENT 3> Pres\n",
      "2   he had stayed home because <EVENT 3>          Past\n",
      "3   it was raining                                Pres\n"
     ]
    }
   ],
   "source": [
    "text = \"She walks to school, but he had stayed home because it was raining.\"\n",
    "# Add import for spacy if not already present\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Labels for clause types\n",
    "SUB_DEPS = {\"advcl\", \"ccomp\", \"acl\", \"relcl\", \"xcomp\"}  # subordinate\n",
    "CC_DEP   = \"cc\"                                         # coordinating conj\n",
    "CMP_DEP   = \"ccomp\"                                     # comparative conj\n",
    "\n",
    "def get_tense(tok):\n",
    "    t = tok.morph.get(\"Tense\")\n",
    "    return t[0] if t else \"—\"\n",
    "\n",
    "# Collect independent-clause verbs and subordinate-clause verbs\n",
    "indep = [t for t in doc if t.pos_ == \"VERB\" and t.dep_ in {\"ROOT\",\"conj\"} and t.head.dep_ not in SUB_DEPS]\n",
    "subord = []\n",
    "for root in indep:\n",
    "    for child in root.children:\n",
    "        if child.pos_ == \"VERB\" and child.dep_ in SUB_DEPS:\n",
    "            subord.append(child)\n",
    "\n",
    "# Assign IDs\n",
    "events = indep + subord\n",
    "event_id = {tok: i+1 for i, tok in enumerate(events)}\n",
    "\n",
    "def build_span(root, kind):\n",
    "    \"\"\"\n",
    "    For 'indep', include everything in subtree except:\n",
    "      - punctuation\n",
    "      - cc-subtrees\n",
    "      - cmp-subtrees (comparative conjunctions)\n",
    "      - tokens under SUB_DEPS branches (except their 'mark')\n",
    "    For 'sub', include entire subordinate subtree but drop its 'mark' and punctuation and cc-subtrees.\n",
    "    \"\"\"\n",
    "    exclude = set()\n",
    "    for child in root.children:\n",
    "        if child.dep_ == CC_DEP or child.dep_ == CMP_DEP:\n",
    "            exclude.update(child.subtree)\n",
    "\n",
    "    tokens = []\n",
    "    for t in root.subtree:\n",
    "        if t.is_punct or t in exclude:\n",
    "            continue\n",
    "        if kind == \"indep\":\n",
    "            # find which direct child of root this token descends from\n",
    "            # via ancestors\n",
    "            direct = None\n",
    "            for anc in t.ancestors:\n",
    "                if anc.head == root:\n",
    "                    direct = anc\n",
    "                    break\n",
    "            # if that direct child is a subordinate clause, skip all its tokens except the 'mark'\n",
    "            if direct and direct.dep_ in SUB_DEPS and t.dep_ != \"mark\":\n",
    "                continue\n",
    "        elif kind == \"sub\":\n",
    "            if t.dep_ == \"mark\":\n",
    "                continue\n",
    "        tokens.append(t)\n",
    "\n",
    "    if not tokens:\n",
    "        tokens = [root]\n",
    "\n",
    "    tokens = sorted(tokens, key=lambda x: x.i)\n",
    "    return doc[tokens[0].i : tokens[-1].i + 1]\n",
    "\n",
    "# Build event records\n",
    "records = []\n",
    "for tok in events:\n",
    "    kind = \"indep\" if tok in indep else \"sub\"\n",
    "    span = build_span(tok, kind)\n",
    "    records.append({\n",
    "        \"tok\": tok,\n",
    "        \"span\": span,\n",
    "        \"kind\": kind,\n",
    "        \"tense\": get_tense(tok),\n",
    "        \"id\": event_id[tok]\n",
    "    })\n",
    "\n",
    "# Sort by occurrence\n",
    "records.sort(key=lambda r: r[\"span\"].start_char)\n",
    "\n",
    "# Build subordinate lookup for replacements\n",
    "sub_lookup = {r[\"span\"]: r[\"id\"] for r in records if r[\"kind\"] == \"sub\"}\n",
    "\n",
    "def annotate_indep(span):\n",
    "    text = span.text\n",
    "    base = span.start_char\n",
    "    # find subordinate spans inside this span\n",
    "    inside = [\n",
    "        (sp, sid) for sp, sid in sub_lookup.items()\n",
    "        if sp.start_char >= base and sp.end_char <= span.end_char\n",
    "    ]\n",
    "    # replace in reverse order to keep indexes valid\n",
    "    for sp, sid in sorted(inside, key=lambda x: x[0].start_char, reverse=True):\n",
    "        s = sp.start_char - base\n",
    "        e = sp.end_char   - base\n",
    "        text = text[:s] + f\"<EVENT {sid}>\" + text[e:]\n",
    "    return text.strip()\n",
    "\n",
    "# Output\n",
    "print(f\"{'ID':<3} {'Event Phrase':<45} Tense\")\n",
    "print(\"-\"*65)\n",
    "for rec in records:\n",
    "    phrase = annotate_indep(rec[\"span\"]) if rec[\"kind\"] == \"indep\" else rec[\"span\"].text\n",
    "    print(f\"{rec['id']:<3} {phrase:<45} {rec['tense']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
