#!/usr/bin/env python3
# pos_inference.py
#
# Inference script for the trained POS router model
# 
# COMPATIBILITY: Updated to match pos_router_train.py architecture:
# - EMB_DIM=48 (not 64)
# - Enhanced model with layer normalization, dropout, and temperature scaling
# - Updated POS tag mapping to include "SPACE" instead of "_"
# - Default model is router_combined.pt (trained with --combine flag)
# - Penn Treebank WSJ evaluation support added

import torch
import torch.nn as nn
from datasets import load_dataset
import argparse
import re
import time
import nltk
import json
import os
from collections import defaultdict
from tqdm import tqdm

def load_model_config(config_path):
    """Load model configuration from JSON file."""
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"Configuration file not found: {config_path}")
    
    with open(config_path, 'r') as f:
        config = json.load(f)
    
    print(f"üìã Loaded config: {config['model_name']} - {config['description']}")
    return config

def get_config_path(model_path):
    """Get corresponding config path for a model file."""
    # Replace .pt extension with .json
    config_path = model_path.replace('.pt', '.json')
    if not os.path.exists(config_path):
        # Try same directory
        model_dir = os.path.dirname(model_path)
        model_name = os.path.basename(model_path).replace('.pt', '')
        config_path = os.path.join(model_dir, f"{model_name}.json")
    
    return config_path

# These will be set from config
EMB_DIM = 48
DW_KERNEL = 3  
N_TAGS = 18
MAX_LEN = 64
POS_TAGS = []

class DepthWiseCNNRouter(nn.Module):
    """Configurable POS router with depth-wise convolutions and optional second layer."""
    def __init__(self, vocab_size: int, config: dict):
        super().__init__()
        
        # Extract architecture parameters from config
        arch = config['architecture']
        self.emb_dim = arch['emb_dim']
        self.dw_kernel = arch['dw_kernel']
        self.n_tags = arch['n_tags']
        self.use_second_conv = arch.get('use_second_conv_layer', False) or arch.get('use_two_layers', False)
        self.use_temp_scaling = arch.get('use_temperature_scaling', True)
        dropout_rate = arch.get('dropout_rate', 0.1)
        
        # Embedding layer
        self.emb = nn.Embedding(vocab_size, self.emb_dim, padding_idx=0)
        self.emb_dropout = nn.Dropout(dropout_rate)
        
        # First depth-wise separable Conv layer
        self.dw1 = nn.Conv1d(
            self.emb_dim, self.emb_dim, kernel_size=self.dw_kernel,
            padding=self.dw_kernel // 2,
            groups=self.emb_dim, bias=True
        )
        self.pw1 = nn.Conv1d(self.emb_dim, self.emb_dim, kernel_size=1)
        self.norm1 = nn.LayerNorm(self.emb_dim)
        self.act1 = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout_rate)
        
        # Optional second depth-wise separable Conv layer
        if self.use_second_conv:
            self.dw2 = nn.Conv1d(
                self.emb_dim, self.emb_dim, kernel_size=self.dw_kernel,
                padding=self.dw_kernel // 2,
                groups=self.emb_dim, bias=True
            )
            self.pw2 = nn.Conv1d(self.emb_dim, self.emb_dim, kernel_size=1)
            self.norm2 = nn.LayerNorm(self.emb_dim)
            self.act2 = nn.ReLU()
            self.dropout2 = nn.Dropout(dropout_rate)
        
        # Final classification layer
        self.lin = nn.Linear(self.emb_dim, self.n_tags)
        
        # Temperature parameter for calibration
        if self.use_temp_scaling:
            self.temperature = nn.Parameter(torch.ones(1))
        else:
            self.register_parameter('temperature', None)

    def forward(self, token_ids, mask, use_temperature=False):
        """
        token_ids : [B, T] int64
        mask      : [B, T] bool  (True on real tokens, False on padding)
        use_temperature : bool - whether to apply temperature scaling
        returns   : log-probs  [B, T, N_TAGS]
        """
        x = self.emb(token_ids)               # [B, T, D]
        x = self.emb_dropout(x)
        
        # First conv layer
        x = x.transpose(1, 2)                 # -> [B, D, T]  for Conv1d
        x = self.pw1(self.act1(self.dw1(x)))  # depth-wise + point-wise
        x = x.transpose(1, 2)                 # back to [B, T, D]
        x = self.norm1(x)
        x = self.dropout1(x)
        
        # Optional second conv layer
        if self.use_second_conv:
            x = x.transpose(1, 2)                 # -> [B, D, T]  for Conv1d
            x = self.pw2(self.act2(self.dw2(x)))  # depth-wise + point-wise
            x = x.transpose(1, 2)                 # back to [B, T, D]
            x = self.norm2(x)
            x = self.dropout2(x)
        
        # Final classification
        logits = self.lin(x)                  # [B, T, n_tags]
        
        # Apply temperature scaling if requested and available
        if use_temperature and self.use_temp_scaling and self.temperature is not None:
            logits = logits / self.temperature
        
        # Use ‚àíinf on padding positions so CE ignores them
        logits = logits.masked_fill(~mask.unsqueeze(-1), -1e4)
        return torch.log_softmax(logits, dim=-1)

def build_vocab_from_config(config, model_path=None):
    """Build vocabulary based on configuration, preferring saved vocab JSON if available."""
    vocab_config = config['vocabulary']
    treebanks = vocab_config['treebanks']
    pad_token = vocab_config.get('pad_token', '<PAD>')
    
    # Try to load vocabulary from saved JSON file first (from modular training script)
    if model_path:
        model_dir = os.path.dirname(model_path)
        model_name = os.path.basename(model_path).replace('.pt', '')
        vocab_json_path = os.path.join(model_dir, f"{model_name}_vocab.json")
        
        if os.path.exists(vocab_json_path):
            try:
                print(f"üìö Loading vocabulary from saved JSON: {vocab_json_path}")
                with open(vocab_json_path, 'r', encoding='utf-8') as f:
                    vocab_data = json.load(f)
                
                vocab = vocab_data['token_to_id']
                print(f"‚úì Loaded vocabulary size: {len(vocab)}")
                
                # Validate vocab size matches config
                expected_size = vocab_config.get('size') or vocab_config.get('expected_vocab_size')
                if expected_size and abs(len(vocab) - expected_size) > 10:  # Small tolerance
                    print(f"‚ö†Ô∏è  Warning: Vocab size mismatch! Expected {expected_size}, got {len(vocab)}")
                else:
                    print(f"‚úì Vocabulary size matches expected: {len(vocab)}")
                    
                return vocab
                
            except Exception as e:
                print(f"‚ö†Ô∏è  Failed to load vocabulary JSON: {e}")
                print("   Falling back to rebuilding vocabulary...")
    
    # Fallback: rebuild vocabulary from scratch (original behavior)
    print(f"üîß Rebuilding vocabulary from configuration...")
    
    if vocab_config['type'] == 'combined':
        print(f"üî• Loading combined treebanks to rebuild vocabulary: {treebanks}")
        vocab = {pad_token: 0}
        
        for tb in treebanks:
            try:
                ds_train_tb = load_dataset("universal_dependencies", tb, split="train", trust_remote_code=True)
                for ex in ds_train_tb:
                    for tok in ex["tokens"]:
                        if tok not in vocab:
                            vocab[tok] = len(vocab)
                print(f"  ‚úì Processed {tb}: vocab size now {len(vocab)}")
            except Exception as e:
                print(f"  ‚ùå Failed to load {tb}: {e}")
        
        print(f"üéØ Final vocabulary size: {len(vocab)}")
        
        # Validate vocab size if expected size is provided
        expected_size = vocab_config.get('expected_vocab_size')
        if expected_size and abs(len(vocab) - expected_size) > 100:  # Allow some tolerance
            print(f"‚ö†Ô∏è  Warning: Vocab size mismatch! Expected ~{expected_size}, got {len(vocab)}")
        
        return vocab
    
    elif vocab_config['type'] == 'single':
        print(f"üìö Loading single treebank vocabulary: {treebanks[0]}")
        ds_train = load_dataset("universal_dependencies", treebanks[0], split="train", trust_remote_code=True)
        
        vocab = {pad_token: 0}
        for ex in ds_train:
            for tok in ex["tokens"]:
                if tok not in vocab:
                    vocab[tok] = len(vocab)
        
        print(f"üìä Vocabulary size: {len(vocab)}")
        return vocab
    
    elif vocab_config['type'] == 'penn_treebank':
        print(f"üèõÔ∏è  Loading Penn Treebank vocabulary")
        try:
            import nltk
            from nltk.corpus import treebank
            
            # Ensure Penn Treebank is available
            try:
                nltk.data.find('corpora/treebank')
            except LookupError:
                print("Downloading Penn Treebank...")
                nltk.download('treebank', quiet=True)
            
            # Build vocabulary from Penn Treebank
            sents = list(treebank.tagged_sents())
            vocab = {pad_token: 0}
            
            for sent in sents:
                for word, tag in sent:
                    word = word.strip()
                    if word and word not in vocab:
                        vocab[word] = len(vocab)
            
            print(f"üìä Penn Treebank vocabulary size: {len(vocab)}")
            return vocab
            
        except Exception as e:
            print(f"‚ùå Failed to load Penn Treebank: {e}")
            print("üí° Falling back to single UD treebank")
            ds_train = load_dataset("universal_dependencies", "en_ewt", split="train", trust_remote_code=True)
            vocab = {pad_token: 0}
            for ex in ds_train:
                for tok in ex["tokens"]:
                    if tok not in vocab:
                        vocab[tok] = len(vocab)
            return vocab
    
    elif vocab_config['type'] in ['combined_ud', 'combined_ud_penn', 'combined_penn']:
        print(f"üî• Loading {vocab_config['type']} vocabulary: {treebanks}")
        vocab = {pad_token: 0}
        
        # Process UD treebanks
        ud_treebanks = [tb for tb in treebanks if not tb.startswith('penn')]
        for tb in ud_treebanks:
            try:
                ds_train_tb = load_dataset("universal_dependencies", tb, split="train", trust_remote_code=True)
                for ex in ds_train_tb:
                    for tok in ex["tokens"]:
                        if tok not in vocab:
                            vocab[tok] = len(vocab)
                print(f"  ‚úì Processed UD {tb}: vocab size now {len(vocab)}")
            except Exception as e:
                print(f"  ‚ùå Failed to load UD {tb}: {e}")
        
        # If combined_ud_penn or combined_penn, also add Penn Treebank vocabulary
        if vocab_config['type'] in ['combined_ud_penn', 'combined_penn'] and any(tb.startswith('penn') for tb in treebanks):
            print(f"  üèõÔ∏è  Adding Penn Treebank vocabulary...")
            try:
                import nltk
                from nltk.corpus import treebank
                
                # Ensure Penn Treebank is available
                try:
                    nltk.data.find('corpora/treebank')
                except LookupError:
                    print("    Downloading Penn Treebank...")
                    nltk.download('treebank', quiet=True)
                
                # Add Penn Treebank words to existing vocabulary
                sents = list(treebank.tagged_sents())
                for sent in sents:
                    for word, tag in sent:
                        word = word.strip()
                        if word and word not in vocab:
                            vocab[word] = len(vocab)
                
                print(f"  ‚úì Added Penn Treebank: final vocab size {len(vocab)}")
            except Exception as e:
                print(f"  ‚ùå Failed to add Penn Treebank: {e}")
        
        print(f"üéØ Final {vocab_config['type']} vocabulary size: {len(vocab)}")
        
        # Validate vocab size if expected size is provided
        expected_size = vocab_config.get('expected_vocab_size')
        if expected_size and abs(len(vocab) - expected_size) > 100:  # Allow some tolerance
            print(f"‚ö†Ô∏è  Warning: Vocab size mismatch! Expected ~{expected_size}, got {len(vocab)}")
        
        return vocab
    
    elif vocab_config['type'] == 'single_treebank':
        print(f"üìö Loading single treebank vocabulary: {treebanks[0]}")
        ds_train = load_dataset("universal_dependencies", treebanks[0], split="train", trust_remote_code=True)
        
        vocab = {pad_token: 0}
        for ex in ds_train:
            for tok in ex["tokens"]:
                if tok not in vocab:
                    vocab[tok] = len(vocab)
        
        print(f"üìä Vocabulary size: {len(vocab)}")
        return vocab
    
    else:
        raise ValueError(f"Unknown vocabulary type: {vocab_config['type']}. Supported types: 'single', 'combined', 'penn_treebank', 'single_treebank', 'combined_ud', 'combined_ud_penn', 'combined_penn'")

class POSPredictor:
    def __init__(self, model_path, config_path=None):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # Load configuration
        if config_path is None:
            config_path = get_config_path(model_path)
        
        self.config = load_model_config(config_path)
        
        # Set global variables from config for backwards compatibility
        global EMB_DIM, DW_KERNEL, N_TAGS, MAX_LEN, POS_TAGS
        arch = self.config['architecture']
        EMB_DIM = arch['emb_dim']
        DW_KERNEL = arch['dw_kernel']
        N_TAGS = arch['n_tags']
        MAX_LEN = arch['max_len']
        
        # Handle different pos_tags structures
        if isinstance(self.config['pos_tags'], list):
            # Direct list format: ["NOUN", "PUNCT", ...]
            POS_TAGS = self.config['pos_tags']
            self.pos_tags = self.config['pos_tags']
        else:
            # Dictionary format: {"tags": ["NOUN", "PUNCT", ...], "count": 18}
            POS_TAGS = self.config['pos_tags']['tags']
            self.pos_tags = self.config['pos_tags']['tags']
        
        # Store config values as instance variables
        self.max_len = arch['max_len']
        
        # Build vocabulary from config (preferring saved vocab JSON)
        self.vocab = build_vocab_from_config(self.config, model_path)
        
        # Load model with config
        self.model = DepthWiseCNNRouter(len(self.vocab), self.config).to(self.device)
        
        # Load model weights with vocabulary size handling
        checkpoint = torch.load(model_path, map_location=self.device)
        
        # Check for vocabulary size mismatch and handle gracefully
        if 'emb.weight' in checkpoint:
            checkpoint_vocab_size = checkpoint['emb.weight'].shape[0]
            current_vocab_size = len(self.vocab)
            
            if checkpoint_vocab_size != current_vocab_size:
                print(f"‚ö†Ô∏è  Vocabulary size mismatch detected:")
                print(f"   Model was trained with: {checkpoint_vocab_size} tokens")
                print(f"   Current vocabulary has: {current_vocab_size} tokens")
                
                if checkpoint_vocab_size < current_vocab_size:
                    print(f"üîß Truncating vocabulary to match model size...")
                    # Create a smaller vocabulary by keeping only the first N tokens
                    sorted_vocab = sorted(self.vocab.items(), key=lambda x: x[1])
                    truncated_vocab = {token: idx for token, idx in sorted_vocab[:checkpoint_vocab_size]}
                    self.vocab = truncated_vocab
                    print(f"‚úì Vocabulary truncated to {len(self.vocab)} tokens")
                    
                    # Recreate model with correct vocab size
                    self.model = DepthWiseCNNRouter(len(self.vocab), self.config).to(self.device)
                    
                elif checkpoint_vocab_size > current_vocab_size:
                    print(f"üîß Expanding vocabulary to match model size...")
                    # Pad vocabulary with dummy tokens
                    for i in range(current_vocab_size, checkpoint_vocab_size):
                        self.vocab[f"<UNK_{i}>"] = i
                    print(f"‚úì Vocabulary expanded to {len(self.vocab)} tokens")
                    
                    # Recreate model with correct vocab size
                    self.model = DepthWiseCNNRouter(len(self.vocab), self.config).to(self.device)
        
        # Load the state dict
        try:
            self.model.load_state_dict(checkpoint)
            print(f"‚úì Model weights loaded successfully")
        except Exception as e:
            print(f"‚ùå Failed to load model weights: {e}")
            # Try loading with strict=False as fallback
            missing_keys, unexpected_keys = self.model.load_state_dict(checkpoint, strict=False)
            if missing_keys:
                print(f"‚ö†Ô∏è  Missing keys: {missing_keys}")
            if unexpected_keys:
                print(f"‚ö†Ô∏è  Unexpected keys: {unexpected_keys}")
            print(f"‚úì Model weights loaded with strict=False")
        
        self.model.eval()
        
        print(f"‚úì Model loaded on {self.device}")
        print(f"‚úì Vocabulary size: {len(self.vocab)}")
        print(f"‚úì Architecture: {arch['emb_dim']}D, " +
              f"{'2-layer' if arch.get('use_second_conv_layer', False) or arch.get('use_two_layers', False) else '1-layer'} CNN, " +
              f"{'with' if arch.get('use_temperature_scaling', True) else 'no'} temp scaling")

    def tokenize(self, text):
        """Lightweight tokenization that separates punctuation properly."""
        # Split on punctuation while keeping the punctuation as separate tokens
        # This handles cases like "peru." -> ["peru", "."]
        text = text.strip()
        
        # Simple regex to split on word boundaries and punctuation
        tokens = re.findall(r"\w+|[^\w\s]", text)
        
        return [token for token in tokens if token.strip()]
    
    def compare_with_baselines(self, text, expected_tags=None):
        """Compare our model with NLTK and spaCy baselines."""
        tokens = self.tokenize(text)
        if not tokens:
            return {}
            
        results = {"text": text, "tokens": tokens}
        
        # Our model prediction with timing
        start_time = time.time()
        our_predictions = self.predict(text)
        our_time = (time.time() - start_time) * 1000  # Convert to ms
        
        results["our_model"] = {
            "predictions": our_predictions,
            "time_ms": our_time,
            "tokens_per_sec": len(tokens) / (our_time / 1000) if our_time > 0 else 0
        }
        
        # NLTK baseline
        try:
            # Ensure required NLTK data is available
            try:
                nltk.data.find('taggers/averaged_perceptron_tagger')
            except LookupError:
                print("Downloading NLTK POS tagger...")
                nltk.download('averaged_perceptron_tagger', quiet=True)
            
            start_time = time.time()
            nltk_tagged = nltk.pos_tag(tokens)
            nltk_time = (time.time() - start_time) * 1000
            
            # Convert NLTK tags to Universal POS (simplified mapping)
            nltk_universal = []
            for word, tag in nltk_tagged:
                # Simple mapping from Penn Treebank to Universal POS
                if tag.startswith('N'):
                    uni_tag = 'NOUN' if not tag.startswith('NNP') else 'PROPN'
                elif tag.startswith('V'):
                    uni_tag = 'AUX' if tag in ['VBZ', 'VBP', 'VBD', 'VB'] and word.lower() in ['is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had'] else 'VERB'
                elif tag.startswith('J'):
                    uni_tag = 'ADJ'
                elif tag.startswith('R'):
                    uni_tag = 'ADV'
                elif tag in ['DT', 'PDT', 'WDT']:
                    uni_tag = 'DET'
                elif tag in ['IN']:
                    uni_tag = 'ADP'
                elif tag in ['PRP', 'PRP$', 'WP', 'WP$']:
                    uni_tag = 'PRON'
                elif tag in ['CC']:
                    uni_tag = 'CCONJ'
                elif tag in ['CD']:
                    uni_tag = 'NUM'
                elif tag in ['.', ',', ':', ';', '!', '?', '``', "''"]:
                    uni_tag = 'PUNCT'
                else:
                    uni_tag = 'X'
                nltk_universal.append((word, uni_tag))
            
            results["nltk"] = {
                "predictions": nltk_universal,
                "time_ms": nltk_time,
                "tokens_per_sec": len(tokens) / (nltk_time / 1000) if nltk_time > 0 else 0
            }
        except Exception as e:
            results["nltk"] = {"error": str(e)}
        
        # spaCy baseline (if available)
        try:
            import spacy
            nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])
            
            start_time = time.time()
            doc = nlp(text)
            spacy_time = (time.time() - start_time) * 1000
            
            spacy_predictions = [(token.text, token.pos_) for token in doc]
            
            results["spacy"] = {
                "predictions": spacy_predictions,
                "time_ms": spacy_time,
                "tokens_per_sec": len(tokens) / (spacy_time / 1000) if spacy_time > 0 else 0
            }
        except Exception as e:
            results["spacy"] = {"error": f"spaCy not available: {e}"}
        
        # Calculate accuracy if expected tags provided
        if expected_tags:
            expected_dict = {token: pos for token, pos in expected_tags}
            
            for model_name in ["our_model", "nltk", "spacy"]:
                if model_name in results and "predictions" in results[model_name]:
                    predictions = results[model_name]["predictions"]
                    correct = sum(1 for token, pred_pos in predictions 
                                if expected_dict.get(token) == pred_pos)
                    total = len(predictions)
                    results[model_name]["accuracy"] = correct / total if total > 0 else 0
                    results[model_name]["correct"] = correct
                    results[model_name]["total"] = total
        
        return results

    def predict(self, text):
        """Predict POS tags for input text."""
        tokens = self.tokenize(text)
        if not tokens:
            return []
        
        # Convert to token IDs
        token_ids = [self.vocab.get(tok, 0) for tok in tokens][:self.max_len]
        
        # Create tensors
        ids = torch.tensor([token_ids]).to(self.device)
        mask = torch.ones_like(ids, dtype=torch.bool)
        
        # Get predictions (ensure model is in eval mode for dropout/normalization)
        self.model.eval()
        with torch.no_grad():
            logp = self.model(ids, mask, use_temperature=True)  # Use calibrated temperature
            pred_ids = logp.argmax(-1).squeeze(0).cpu().numpy()
        
        # Convert to POS tags
        predictions = []
        for i, (token, pred_id) in enumerate(zip(tokens, pred_ids)):
            if i < len(token_ids):  # Only for actual tokens
                pred_id = int(pred_id)  # Convert numpy int64 to Python int
                pos_tag = self.pos_tags[pred_id] if pred_id < len(self.pos_tags) else "X"
                predictions.append((token, pos_tag))
        
        return predictions
    
    def predict_batch(self, texts, batch_size=512):
        """Predict POS tags for a batch of texts efficiently."""
        if not texts:
            return []
        
        all_predictions = []
        total_batches = (len(texts) + batch_size - 1) // batch_size
        
        print(f"Processing {len(texts)} sentences in {total_batches} batches of {batch_size}...")
        
        for i in tqdm(range(0, len(texts), batch_size), desc="Processing batches"):
            batch_texts = texts[i:i + batch_size]
            batch_predictions = []
            
            # Tokenize all texts in batch
            batch_tokens = [self.tokenize(text) for text in batch_texts]
            
            # Find max length for padding
            max_len = max(len(tokens) for tokens in batch_tokens) if batch_tokens else 0
            max_len = min(max_len, self.max_len)  # Respect model's max length
            
            # Create batch tensors
            batch_ids = torch.zeros(len(batch_tokens), max_len, dtype=torch.long)
            batch_mask = torch.zeros(len(batch_tokens), max_len, dtype=torch.bool)
            
            for j, tokens in enumerate(batch_tokens):
                token_ids = [self.vocab.get(tok, 0) for tok in tokens[:max_len]]
                n = len(token_ids)
                batch_ids[j, :n] = torch.tensor(token_ids)
                batch_mask[j, :n] = True
            
            # Move to device
            batch_ids = batch_ids.to(self.device)
            batch_mask = batch_mask.to(self.device)
            
            # Get predictions (ensure model is in eval mode)
            self.model.eval()
            with torch.no_grad():
                logp = self.model(batch_ids, batch_mask, use_temperature=True)
                pred_ids = logp.argmax(-1).cpu().numpy()
            
            # Convert to POS tags
            for j, (tokens, preds) in enumerate(zip(batch_tokens, pred_ids)):
                predictions = []
                for k, (token, pred_id) in enumerate(zip(tokens, preds)):
                    if k < len(tokens) and batch_mask[j, k].item():
                        pred_id = int(pred_id)  # Convert numpy int64 to Python int
                        pos_tag = self.pos_tags[pred_id] if pred_id < len(self.pos_tags) else "X"
                        predictions.append((token, pos_tag))
                batch_predictions.append(predictions)
            
            all_predictions.extend(batch_predictions)
        
        return all_predictions
    
    def benchmark_batch(self, texts, batch_size=512):
        """Run comprehensive batch benchmark against baselines."""
        print(f"\nüöÄ Large-Scale Batch Benchmark")
        print(f"Dataset size: {len(texts)} sentences")
        print(f"Batch size: {batch_size}")
        print("=" * 60)
        
        results = {}
        total_tokens = sum(len(self.tokenize(text)) for text in texts)
        
        # Our model
        print("\n‚ö° Testing our model...")
        start_time = time.time()
        our_predictions = self.predict_batch(texts, batch_size)
        our_time = time.time() - start_time
        
        results["our_model"] = {
            "time": our_time,
            "predictions": our_predictions,
            "sentences_per_sec": len(texts) / our_time,
            "tokens_per_sec": total_tokens / our_time
        }
        
        # NLTK baseline
        print("\nüìö Testing NLTK...")
        try:
            # Ensure NLTK data is available
            try:
                nltk.data.find('taggers/averaged_perceptron_tagger')
            except LookupError:
                print("Downloading NLTK POS tagger...")
                nltk.download('averaged_perceptron_tagger', quiet=True)
            
            start_time = time.time()
            nltk_predictions = []
            for text in tqdm(texts, desc="NLTK processing"):
                tokens = self.tokenize(text)
                nltk_tagged = nltk.pos_tag(tokens)
                
                # Convert to Universal POS
                universal_tags = []
                for word, tag in nltk_tagged:
                    if tag.startswith('N'):
                        uni_tag = 'NOUN' if not tag.startswith('NNP') else 'PROPN'
                    elif tag.startswith('V'):
                        uni_tag = 'AUX' if tag in ['VBZ', 'VBP', 'VBD', 'VB'] and word.lower() in ['is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had'] else 'VERB'
                    elif tag.startswith('J'):
                        uni_tag = 'ADJ'
                    elif tag.startswith('R'):
                        uni_tag = 'ADV'
                    elif tag in ['DT', 'PDT', 'WDT']:
                        uni_tag = 'DET'
                    elif tag in ['IN']:
                        uni_tag = 'ADP'
                    elif tag in ['PRP', 'PRP$', 'WP', 'WP$']:
                        uni_tag = 'PRON'
                    elif tag in ['CC']:
                        uni_tag = 'CCONJ'
                    elif tag in ['CD']:
                        uni_tag = 'NUM'
                    elif tag in ['.', ',', ':', ';', '!', '?', '``', "''"]:
                        uni_tag = 'PUNCT'
                    else:
                        uni_tag = 'X'
                    universal_tags.append((word, uni_tag))
                
                nltk_predictions.append(universal_tags)
            
            nltk_time = time.time() - start_time
            results["nltk"] = {
                "time": nltk_time,
                "predictions": nltk_predictions,
                "sentences_per_sec": len(texts) / nltk_time,
                "tokens_per_sec": total_tokens / nltk_time
            }
        except Exception as e:
            results["nltk"] = {"error": str(e)}
        
        # spaCy baseline
        print("\nüåø Testing spaCy...")
        try:
            import spacy
            nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])
            
            start_time = time.time()
            spacy_predictions = []
            
            # Process in spaCy batches for efficiency
            spacy_batch_size = 100
            for i in tqdm(range(0, len(texts), spacy_batch_size), desc="spaCy processing"):
                batch_texts = texts[i:i + spacy_batch_size]
                docs = list(nlp.pipe(batch_texts))
                
                for doc in docs:
                    predictions = [(token.text, token.pos_) for token in doc]
                    spacy_predictions.append(predictions)
            
            spacy_time = time.time() - start_time
            results["spacy"] = {
                "time": spacy_time,
                "predictions": spacy_predictions,
                "sentences_per_sec": len(texts) / spacy_time,
                "tokens_per_sec": total_tokens / spacy_time
            }
        except Exception as e:
            results["spacy"] = {"error": f"spaCy not available: {e}"}
        
        return results
    
    def stress_test(self, max_batch_size=4096):
        """Run extreme scale stress testing to find maximum throughput."""
        print(f"\nüî• EXTREME SCALE STRESS TEST")
        print("=" * 60)
        print("Testing maximum throughput with large datasets and batch sizes")
        
        # Test configurations
        test_configs = [
            {"sentences": 10000, "batch_sizes": [512, 1024, 2048]},
            {"sentences": 50000, "batch_sizes": [1024, 2048, 4096]},
            {"sentences": 100000, "batch_sizes": [2048, 4096]},
            {"sentences": 250000, "batch_sizes": [4096]},
        ]
        
        # Add max batch size if different
        if max_batch_size > 4096:
            test_configs.append({
                "sentences": 500000, 
                "batch_sizes": [max_batch_size]
            })
        
        results = []
        
        for config in test_configs:
            num_sentences = config["sentences"]
            batch_sizes = config["batch_sizes"]
            
            print(f"\nüéØ Testing {num_sentences:,} sentences:")
            print("-" * 40)
            
            # Load sentences for this test
            test_sentences = load_test_sentences(num_sentences, "en_ewt")  # Use larger en_ewt
            actual_sentences = len(test_sentences)
            total_tokens = sum(len(self.tokenize(text)) for text in test_sentences)
            
            print(f"üìä Dataset: {actual_sentences:,} sentences, {total_tokens:,} tokens")
            
            for batch_size in batch_sizes:
                try:
                    print(f"\n‚ö° Batch size {batch_size:,}:")
                    
                    # Test our model only (skip baselines for speed)
                    start_time = time.time()
                    predictions = self.predict_batch(test_sentences, batch_size)
                    total_time = time.time() - start_time
                    
                    sent_per_sec = actual_sentences / total_time
                    tok_per_sec = total_tokens / total_time
                    
                    result = {
                        "sentences": actual_sentences,
                        "batch_size": batch_size,
                        "time": total_time,
                        "sent_per_sec": sent_per_sec,
                        "tok_per_sec": tok_per_sec,
                        "tokens": total_tokens
                    }
                    results.append(result)
                    
                    print(f"  Time: {total_time:.1f}s")
                    print(f"  Sentences/sec: {sent_per_sec:.1f}")
                    print(f"  Tokens/sec: {tok_per_sec:.0f}")
                    print(f"  GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.1f}GB" if torch.cuda.is_available() else "  GPU: N/A")
                    
                except Exception as e:
                    print(f"  ‚ùå Failed: {e}")
                    break  # GPU probably ran out of memory
        
        # Find peak performance
        best_result = max(results, key=lambda x: x["tok_per_sec"])
        
        print(f"\nüèÜ PEAK PERFORMANCE ACHIEVED:")
        print("=" * 50)
        print(f"üöÄ Maximum throughput: {best_result['tok_per_sec']:.0f} tokens/sec")
        print(f"üìä Configuration: {best_result['sentences']:,} sentences, batch size {best_result['batch_size']:,}")
        print(f"‚è±Ô∏è  Total time: {best_result['time']:.1f}s")
        print(f"üéØ Sentences/sec: {best_result['sent_per_sec']:.1f}")
        
        # Performance scaling analysis
        print(f"\nüìà SCALING ANALYSIS:")
        print("-" * 30)
        batch_size_analysis = {}
        for result in results:
            bs = result["batch_size"]
            if bs not in batch_size_analysis:
                batch_size_analysis[bs] = []
            batch_size_analysis[bs].append(result["tok_per_sec"])
        
        for batch_size in sorted(batch_size_analysis.keys()):
            avg_throughput = sum(batch_size_analysis[batch_size]) / len(batch_size_analysis[batch_size])
            print(f"Batch {batch_size:,}: {avg_throughput:.0f} tokens/sec average")
        
        # Efficiency metrics
        print(f"\nüí° EFFICIENCY METRICS:")
        print(f"‚Ä¢ Peak tokens/sec per parameter: {best_result['tok_per_sec'] / 64000:.1f}")
        print(f"‚Ä¢ Model parameters: ~64K")
        print(f"‚Ä¢ Memory efficiency: Excellent (batch processing)")
        print(f"‚Ä¢ GPU utilization: High (optimized pipeline)")
        
        return results
    
    def optimize_batch_size(self, config_file="batch_config.json", test_sentences=5000, fine_tune=True):
        """Find optimal batch size for this GPU and save to config file."""
        print(f"\nüéØ BATCH SIZE OPTIMIZATION")
        print("=" * 60)
        print(f"Finding optimal batch size for your GPU...")
        print(f"Test dataset: {test_sentences:,} sentences")
        
        # Load test sentences
        sentences = load_test_sentences(test_sentences, "en_ewt")
        actual_sentences = len(sentences)
        total_tokens = sum(len(self.tokenize(text)) for text in sentences)
        
        print(f"üìä Testing with {actual_sentences:,} sentences ({total_tokens:,} tokens)")
        
        # Test batch sizes - start with powers of 2, then refine
        initial_batch_sizes = [64, 128, 256, 512, 1024, 2048, 4096, 8192]
        
        results = []
        best_throughput = 0
        best_batch_size = 512
        
        print(f"\nüîç Phase 1: Testing standard batch sizes")
        print("-" * 40)
        
        for batch_size in initial_batch_sizes:
            try:
                print(f"Testing batch size {batch_size:,}...", end=" ")
                
                # Warm up
                warm_sentences = sentences[:min(1000, len(sentences))]
                _ = self.predict_batch(warm_sentences, batch_size)
                
                # Actual test
                start_time = time.time()
                _ = self.predict_batch(sentences, batch_size)
                test_time = time.time() - start_time
                
                throughput = total_tokens / test_time
                results.append({
                    "batch_size": batch_size,
                    "time": test_time,
                    "throughput": throughput,
                    "tokens_per_sec": throughput
                })
                
                print(f"{throughput:.0f} tokens/sec")
                
                if throughput > best_throughput:
                    best_throughput = throughput
                    best_batch_size = batch_size
                
                # Check GPU memory after each test
                if torch.cuda.is_available():
                    memory_used = torch.cuda.memory_allocated() / 1024**3
                    if memory_used > 10:  # If using > 10GB, probably close to limit
                        print(f"  (High GPU memory usage: {memory_used:.1f}GB)")
                
            except Exception as e:
                print(f"FAILED ({e})")
                print(f"  GPU memory limit reached at batch size {batch_size:,}")
                break
        
        # Phase 2: Coarse refinement around the best batch size
        if results:
            print(f"\nüéØ Phase 2: Coarse refinement around optimal batch size")
            print("-" * 40)
            
            # Test sizes around the best one
            refinement_range = [
                int(best_batch_size * 0.75),
                int(best_batch_size * 1.25),
                int(best_batch_size * 1.5)
            ]
            
            for batch_size in refinement_range:
                if batch_size <= 64 or batch_size in [r["batch_size"] for r in results]:
                    continue  # Skip if too small or already tested
                
                try:
                    print(f"Testing batch size {batch_size:,}...", end=" ")
                    
                    start_time = time.time()
                    _ = self.predict_batch(sentences, batch_size)
                    test_time = time.time() - start_time
                    
                    throughput = total_tokens / test_time
                    results.append({
                        "batch_size": batch_size,
                        "time": test_time,
                        "throughput": throughput,
                        "tokens_per_sec": throughput
                    })
                    
                    print(f"{throughput:.0f} tokens/sec")
                    
                    if throughput > best_throughput:
                        best_throughput = throughput
                        best_batch_size = batch_size
                
                except Exception as e:
                    print(f"FAILED ({e})")
        
        # Phase 3: Fine-grained tuning (increments of 10-50)
        if results and fine_tune:
            # Update best batch size from all results so far
            results.sort(key=lambda x: x["throughput"], reverse=True)
            current_best = results[0]["batch_size"]
            
            print(f"\nüî¨ Phase 3: Fine-grained tuning around {current_best:,}")
            print("-" * 40)
            
            # Determine step size based on batch size magnitude
            if current_best >= 4096:
                step_size = 50  # For large batch sizes, step by 50
            elif current_best >= 1024:
                step_size = 20  # For medium batch sizes, step by 20
            else:
                step_size = 10  # For small batch sizes, step by 10
            
            # Test range: ¬±10% around current best in fine increments
            range_size = max(100, int(current_best * 0.1))  # At least ¬±100
            fine_range = []
            
            # Generate fine-grained test points
            for offset in range(-range_size, range_size + 1, step_size):
                test_batch = current_best + offset
                if test_batch >= 64 and test_batch not in [r["batch_size"] for r in results]:
                    fine_range.append(test_batch)
            
            # Sort by distance from current best (test closest first)
            fine_range.sort(key=lambda x: abs(x - current_best))
            
            print(f"Testing {len(fine_range)} fine-grained batch sizes (step: {step_size})...")
            
            consecutive_failures = 0
            max_failures = 3  # Stop if 3 consecutive OOM errors
            
            for i, batch_size in enumerate(fine_range):
                try:
                    print(f"Fine-tuning {batch_size:,}...", end=" ")
                    
                    start_time = time.time()
                    _ = self.predict_batch(sentences, batch_size)
                    test_time = time.time() - start_time
                    
                    throughput = total_tokens / test_time
                    results.append({
                        "batch_size": batch_size,
                        "time": test_time,
                        "throughput": throughput,
                        "tokens_per_sec": throughput
                    })
                    
                    print(f"{throughput:.0f} tokens/sec")
                    consecutive_failures = 0  # Reset failure count
                    
                    if throughput > best_throughput:
                        best_throughput = throughput
                        best_batch_size = batch_size
                        print(f"  üéØ New best!")
                
                except Exception as e:
                    print(f"FAILED ({e})")
                    consecutive_failures += 1
                    if consecutive_failures >= max_failures:
                        print(f"  Stopping fine-tuning after {max_failures} consecutive failures")
                        break
                
                # Show progress every 5 tests
                if (i + 1) % 5 == 0:
                    current_best_result = max(results, key=lambda x: x["throughput"])
                    print(f"  Progress: {i+1}/{len(fine_range)}, current best: {current_best_result['batch_size']:,} ({current_best_result['throughput']:.0f} tok/s)")
        
        # Sort results by throughput and get final optimal config
        results.sort(key=lambda x: x["throughput"], reverse=True)
        optimal_result = results[0] if results else None
        
        # Gather system info
        gpu_info = "N/A"
        gpu_memory = "N/A"
        if torch.cuda.is_available():
            gpu_info = torch.cuda.get_device_name(0)
            gpu_memory = f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB"
        
        # Save configuration
        config = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "gpu_info": gpu_info,
            "gpu_memory": gpu_memory,
            "test_sentences": actual_sentences,
            "test_tokens": total_tokens,
            "optimal_batch_size": optimal_result["batch_size"] if optimal_result else 512,
            "optimal_throughput": optimal_result["throughput"] if optimal_result else 0,
            "all_results": results[:5],  # Keep top 5 results
            "model_info": {
                "vocab_size": len(self.vocab),
                "parameters": "~64K",
                "architecture": "DepthWiseCNN"
            }
        }
        
        with open(config_file, 'w') as f:
            json.dump(config, f, indent=2)
        
        # Display results
        print(f"\nüèÜ OPTIMIZATION COMPLETE!")
        print("=" * 50)
        print(f"üöÄ Optimal batch size: {config['optimal_batch_size']:,}")
        print(f"üìà Peak throughput: {config['optimal_throughput']:.0f} tokens/sec")
        print(f"üíæ Configuration saved to: {config_file}")
        
        print(f"\nüìä TOP PERFORMING BATCH SIZES:")
        print("-" * 40)
        for i, result in enumerate(results[:5], 1):
            print(f"{i}. Batch {result['batch_size']:,}: {result['throughput']:.0f} tokens/sec")
        
        print(f"\nüí° RECOMMENDATIONS:")
        print(f"‚Ä¢ Use batch size {config['optimal_batch_size']:,} for maximum throughput")
        print(f"‚Ä¢ Add --batch-size {config['optimal_batch_size']:,} to your commands")
        print(f"‚Ä¢ Your GPU: {gpu_info}")
        print(f"‚Ä¢ Memory available: {gpu_memory}")
        
        # Show improvement from fine-tuning if applicable
        if fine_tune and len(results) > 8:  # If we did fine-tuning
            coarse_best = max(results[8:], key=lambda x: x["throughput"]) if len(results) > 8 else None
            fine_best = results[0]
            if coarse_best and fine_best["throughput"] > coarse_best["throughput"]:
                improvement = ((fine_best["throughput"] - coarse_best["throughput"]) / coarse_best["throughput"]) * 100
                print(f"üî¨ Fine-tuning improvement: +{improvement:.1f}% throughput")
                print(f"   ({coarse_best['throughput']:.0f} ‚Üí {fine_best['throughput']:.0f} tokens/sec)")
        
        return config

def load_optimal_batch_size(config_file="batch_config.json"):
    """Load optimal batch size from config file."""
    if not os.path.exists(config_file):
        return None
    
    try:
        with open(config_file, 'r') as f:
            config = json.load(f)
        
        # Validate config has the expected structure
        if 'optimal_batch_size' in config and 'gpu_info' in config:
            return config
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Warning: Could not load config from {config_file}: {e}")
    
    return None

def load_test_sentences(num_sentences=2000, treebank="en_ewt"):
    """Load test sentences from Universal Dependencies dataset."""
    print(f"üì• Loading {num_sentences} test sentences from {treebank}...")
    
    try:
        # Try validation set first
        dataset = load_dataset("universal_dependencies", treebank, split="validation", trust_remote_code=True)
        
        # Extract raw sentences
        sentences = []
        for i, example in enumerate(dataset):
            # Join tokens to form original sentence
            sentence = " ".join(example["tokens"])
            sentences.append(sentence)
        
        print(f"üìä Validation set has {len(sentences)} sentences")
        
        # If we need more sentences than validation set has, add from train set
        if len(sentences) < num_sentences:
            print(f"üîÑ Need {num_sentences - len(sentences)} more sentences, loading from train set...")
            train_dataset = load_dataset("universal_dependencies", treebank, split="train", trust_remote_code=True)
            
            needed = num_sentences - len(sentences)
            for i, example in enumerate(train_dataset):
                if i >= needed:
                    break
                sentence = " ".join(example["tokens"])
                sentences.append(sentence)
        
        # If still not enough, fall back to generated sentences
        if len(sentences) < num_sentences:
            print(f"üîÑ Still need {num_sentences - len(sentences)} more sentences, generating...")
            generated = generate_test_sentences(num_sentences - len(sentences))
            sentences.extend(generated)
        
        print(f"‚úÖ Total loaded: {len(sentences)} sentences")
        return sentences[:num_sentences]
    
    except Exception as e:
        print(f"‚ùå Failed to load dataset: {e}")
        # Fallback to generated sentences
        print("üîÑ Using generated test sentences...")
        return generate_test_sentences(num_sentences)

def generate_test_sentences(num_sentences=2000):
    """Generate test sentences for benchmarking."""
    import random
    
    # Templates for different sentence types
    templates = [
        "The {adj} {noun} {verb} {adv}.",
        "{name} {verb} to the {place} {adv}.",
        "I {verb} that {pronoun} {verb} very {adj}.",
        "The {adj} {noun} and the {adj2} {noun2} {verb} together.",
        "{name} {verb} {noun} from {place} to {place2}.",
        "When {pronoun} {verb}, {pronoun2} {verb2} {adv}.",
        "The {adj} {noun} {verb} because {pronoun} {verb2} {adj2}."
    ]
    
    # Word lists
    adjectives = ["quick", "brown", "lazy", "smart", "big", "small", "fast", "slow", "good", "bad"]
    nouns = ["fox", "dog", "cat", "man", "woman", "child", "book", "car", "house", "tree"]
    verbs = ["runs", "jumps", "walks", "reads", "writes", "sleeps", "eats", "drinks", "works", "plays"]
    adverbs = ["quickly", "slowly", "carefully", "loudly", "quietly", "happily", "sadly", "often", "never", "always"]
    names = ["John", "Mary", "Alice", "Bob", "Charlie", "Diana", "Emma", "Frank", "Grace", "Henry"]
    places = ["park", "store", "school", "office", "home", "library", "restaurant", "beach", "mountain", "city"]
    pronouns = ["he", "she", "they", "we", "I", "you"]
    
    sentences = []
    random.seed(42)  # For reproducibility
    
    for _ in range(num_sentences):
        template = random.choice(templates)
        sentence = template.format(
            adj=random.choice(adjectives),
            adj2=random.choice(adjectives),
            noun=random.choice(nouns),
            noun2=random.choice(nouns),
            verb=random.choice(verbs),
            verb2=random.choice(verbs),
            adv=random.choice(adverbs),
            name=random.choice(names),
            place=random.choice(places),
            place2=random.choice(places),
            pronoun=random.choice(pronouns),
            pronoun2=random.choice(pronouns)
        )
        sentences.append(sentence)
    
    return sentences

def print_batch_results(results):
    """Print formatted batch benchmark results."""
    print(f"\n{'='*80}")
    print("üìä LARGE-SCALE BATCH BENCHMARK RESULTS")
    print(f"{'='*80}")
    
    # Performance summary table
    print(f"\n‚ö° PERFORMANCE SUMMARY:")
    print(f"{'Model':<12} {'Time (s)':<10} {'Sent/sec':<10} {'Tokens/sec':<12} {'Status'}")
    print("-" * 60)
    
    for model_name in ["our_model", "nltk", "spacy"]:
        if model_name in results:
            data = results[model_name]
            if "error" in data:
                print(f"{model_name:<12} {'ERROR':<10} {'ERROR':<10} {'ERROR':<12} {data['error'][:20]}")
            else:
                time_str = f"{data['time']:.1f}"
                sent_str = f"{data['sentences_per_sec']:.1f}"
                tok_str = f"{data['tokens_per_sec']:.0f}"
                print(f"{model_name:<12} {time_str:<10} {sent_str:<10} {tok_str:<12} ‚úì")
    
    # Speed comparisons
    if "our_model" in results and "time" in results["our_model"]:
        our_time = results["our_model"]["time"]
        our_sent_speed = results["our_model"]["sentences_per_sec"]
        our_tok_speed = results["our_model"]["tokens_per_sec"]
        
        print(f"\nüèÜ SPEED COMPARISONS (vs Our Model):")
        print("-" * 40)
        
        for model_name in ["nltk", "spacy"]:
            if model_name in results and "time" in results[model_name]:
                their_time = results[model_name]["time"]
                speedup = their_time / our_time
                if speedup > 1:
                    print(f"{model_name.upper()}: {speedup:.1f}x SLOWER")
                else:
                    print(f"{model_name.upper()}: {1/speedup:.1f}x FASTER")
        
        print(f"\nüìà THROUGHPUT DETAILS:")
        print(f"Our Model:")
        print(f"  ‚Ä¢ Sentences/second: {our_sent_speed:.1f}")
        print(f"  ‚Ä¢ Tokens/second: {our_tok_speed:.0f}")
        print(f"  ‚Ä¢ Total time: {our_time:.1f}s")
        
        # Model efficiency
        print(f"\nüí° EFFICIENCY INSIGHTS:")
        print(f"‚Ä¢ Our tiny CNN model achieves {our_tok_speed:.0f} tokens/sec")
        print(f"‚Ä¢ Model size: ~64K parameters")
        print(f"‚Ä¢ Memory efficient: Uses AMP (mixed precision)")
        print(f"‚Ä¢ GPU optimized: Batch processing with cuDNN")

def print_comparison_results(results):
    """Print formatted comparison results."""
    print(f"\n{'='*80}")
    print(f"üèÜ POS TAGGER COMPARISON")
    print(f"{'='*80}")
    print(f"Input: {results['text']}")
    print(f"Tokens: {len(results['tokens'])}")
    
    # Performance table
    print(f"\nüìä PERFORMANCE COMPARISON:")
    print(f"{'Model':<12} {'Time (ms)':<10} {'Tokens/sec':<12} {'Accuracy':<10} {'Status'}")
    print("-" * 60)
    
    for model_name in ["our_model", "nltk", "spacy"]:
        if model_name in results:
            data = results[model_name]
            if "error" in data:
                print(f"{model_name:<12} {'N/A':<10} {'N/A':<12} {'N/A':<10} {data['error']}")
            else:
                time_str = f"{data['time_ms']:.1f}"
                speed_str = f"{data['tokens_per_sec']:.0f}"
                acc_str = f"{data.get('accuracy', 0)*100:.1f}%" if 'accuracy' in data else "N/A"
                status = "‚úì"
                print(f"{model_name:<12} {time_str:<10} {speed_str:<12} {acc_str:<10} {status}")
    
    # Detailed predictions
    print(f"\nüîç DETAILED PREDICTIONS:")
    for model_name in ["our_model", "nltk", "spacy"]:
        if model_name in results and "predictions" in results[model_name]:
            print(f"\n{model_name.upper()}:")
            predictions = results[model_name]["predictions"]
            for token, pos in predictions:
                print(f"  {token:15} -> {pos}")

# ============================================================================
# PENN TREEBANK EVALUATION FUNCTIONS
# ============================================================================

def penn_to_universal_mapping():
    """
    Comprehensive mapping from Penn Treebank POS tags to Universal POS tags.
    Based on the Universal Dependencies mapping.
    """
    return {
        # Nouns
        'NN': 'NOUN', 'NNS': 'NOUN', 'NNP': 'PROPN', 'NNPS': 'PROPN',
        
        # Verbs  
        'VB': 'VERB', 'VBD': 'VERB', 'VBG': 'VERB', 'VBN': 'VERB', 'VBP': 'VERB', 'VBZ': 'VERB',
        
        # Auxiliaries (context-dependent, but these are common aux verbs)
        'MD': 'AUX',  # Modal auxiliaries
        
        # Adjectives
        'JJ': 'ADJ', 'JJR': 'ADJ', 'JJS': 'ADJ',
        
        # Adverbs
        'RB': 'ADV', 'RBR': 'ADV', 'RBS': 'ADV', 'WRB': 'ADV',
        
        # Pronouns
        'PRP': 'PRON', 'PRP$': 'PRON', 'WP': 'PRON', 'WP$': 'PRON',
        
        # Determiners
        'DT': 'DET', 'PDT': 'DET', 'WDT': 'DET',
        
        # Prepositions/Adpositions
        'IN': 'ADP',
        
        # Coordinating conjunctions
        'CC': 'CCONJ',
        
        # Subordinating conjunctions  
        # Note: IN can be either ADP or SCONJ - we default to ADP above
        
        # Numbers
        'CD': 'NUM',
        
        # Particles
        'RP': 'PART', 'TO': 'PART',
        
        # Interjections
        'UH': 'INTJ',
        
        # Symbols and punctuation
        'SYM': 'SYM',
        '.': 'PUNCT', ',': 'PUNCT', ':': 'PUNCT', ';': 'PUNCT', 
        '!': 'PUNCT', '?': 'PUNCT', '``': 'PUNCT', "''": 'PUNCT',
        '(': 'PUNCT', ')': 'PUNCT', '"': 'PUNCT', '#': 'PUNCT', '$': 'PUNCT',
        
        # Other/Unknown
        'FW': 'X',     # Foreign words
        'LS': 'X',     # List markers
        'POS': 'PART', # Possessive endings
        'EX': 'PRON',  # Existential there
        
        # Default for any unmapped tags
        'X': 'X'
    }

def convert_penn_to_universal(penn_tags):
    """Convert Penn Treebank tags to Universal POS tags."""
    mapping = penn_to_universal_mapping()
    universal_tags = []
    
    for word, penn_tag in penn_tags:
        # Handle compound tags (like PRP$ -> PRON)
        base_tag = penn_tag.split('-')[0].split('+')[0]  # Remove suffixes like -TMP, -1, etc.
        universal_tag = mapping.get(base_tag, 'X')
        
        # Special handling for auxiliaries (context-dependent)
        if base_tag in ['VBZ', 'VBP', 'VBD', 'VB'] and word.lower() in {
            'be', 'am', 'is', 'are', 'was', 'were', 'being', 'been',
            'have', 'has', 'had', 'having', 'do', 'does', 'did'
        }:
            universal_tag = 'AUX'
        
        universal_tags.append((word, universal_tag))
    
    return universal_tags

def load_penn_treebank_test():
    """
    Load Penn Treebank test data available through NLTK.
    
    Note: NLTK's Penn Treebank corpus contains a sample of the full corpus.
    For full WSJ sections 22-24 evaluation, you need the complete LDC Penn Treebank.
    
    Returns:
        List of (sentence, tags) tuples where tags are in Universal POS format
    """
    try:
        # Ensure Penn Treebank data is available
        try:
            nltk.data.find('corpora/treebank')
        except LookupError:
            print("Downloading Penn Treebank sample...")
            nltk.download('treebank', quiet=True)
        
        from nltk.corpus import treebank
        
        print("Loading Penn Treebank sample from NLTK...")
        
        # Get all tagged sentences
        penn_sentences = list(treebank.tagged_sents())
        
        # Convert to universal tags and prepare test data
        test_data = []
        
        for sent in penn_sentences:
            if len(sent) == 0:
                continue
                
            # Extract words and penn tags
            words = [word for word, tag in sent]
            penn_tags = [(word, tag) for word, tag in sent]
            
            # Convert to universal tags
            universal_tags = convert_penn_to_universal(penn_tags)
            expected_tags = [tag for word, tag in universal_tags]
            
            # Create sentence text
            sentence_text = " ".join(words)
            
            test_data.append((sentence_text, expected_tags))
        
        print(f"‚úÖ Loaded {len(test_data)} Penn Treebank sentences")
        print(f"üìä Total tokens: {sum(len(tags) for _, tags in test_data)}")
        
        return test_data
        
    except Exception as e:
        print(f"‚ùå Error loading Penn Treebank: {e}")
        print("üí° This uses NLTK's sample of Penn Treebank. For full WSJ sections 22-24:")
        print("   1. Obtain the complete Penn Treebank from LDC")
        print("   2. Use load_wsj_sections_22_24() function below")
        return []

def load_wsj_sections_22_24(penn_treebank_path):
    """
    Load Wall Street Journal sections 22-24 from the complete Penn Treebank.
    
    This function is for users who have access to the complete LDC Penn Treebank.
    
    Args:
        penn_treebank_path: Path to the Penn Treebank root directory
        
    Returns:
        List of (sentence, tags) tuples for WSJ sections 22-24
        
    Example usage:
        # If you have the complete Penn Treebank from LDC:
        test_data = load_wsj_sections_22_24("/path/to/penn-treebank-rel3/parsed/mrg/wsj/")
    """
    import os
    import glob
    
    if not os.path.exists(penn_treebank_path):
        raise FileNotFoundError(f"Penn Treebank path not found: {penn_treebank_path}")
    
    test_data = []
    
    # WSJ sections 22, 23, 24 are the standard test set
    test_sections = ['22', '23', '24']
    
    print(f"Loading WSJ sections {test_sections} from {penn_treebank_path}")
    
    for section in test_sections:
        section_path = os.path.join(penn_treebank_path, section)
        if not os.path.exists(section_path):
            print(f"‚ö†Ô∏è  Section {section} not found at {section_path}")
            continue
            
        # Find all .mrg files in this section
        mrg_files = glob.glob(os.path.join(section_path, "*.mrg"))
        
        for mrg_file in mrg_files:
            print(f"Processing {mrg_file}...")
            
            # Parse the .mrg file (simplified parser)
            # Note: Full implementation would need a proper treebank parser
            # This is a placeholder for the actual implementation
            try:
                sentences = parse_mrg_file(mrg_file)
                for sentence, penn_tags in sentences:
                    universal_tags = convert_penn_to_universal(penn_tags)
                    expected_tags = [tag for word, tag in universal_tags]
                    test_data.append((sentence, expected_tags))
            except Exception as e:
                print(f"Error parsing {mrg_file}: {e}")
    
    print(f"‚úÖ Loaded {len(test_data)} sentences from WSJ sections {test_sections}")
    return test_data

def parse_mrg_file(mrg_file):
    """
    Parse a .mrg file from Penn Treebank.
    
    Note: This is a simplified implementation. A complete implementation
    would use the NLTK tree parsing functionality or a dedicated parser.
    """
    # Placeholder implementation
    # In practice, you would use:
    # from nltk.tree import Tree
    # Or use the full Penn Treebank parsing utilities
    
    print(f"‚ö†Ô∏è  parse_mrg_file is a placeholder. Implement full .mrg parsing for complete WSJ evaluation.")
    return []

def evaluate_on_penn_treebank(predictor, test_data=None, max_sentences=None):
    """
    Evaluate the model on Penn Treebank test data.
    
    Args:
        predictor: POSPredictor instance
        test_data: Pre-loaded test data, or None to load from NLTK
        max_sentences: Limit number of sentences for testing
        
    Returns:
        Dictionary with evaluation results
    """
    if test_data is None:
        test_data = load_penn_treebank_test()
    
    if not test_data:
        return {"error": "No Penn Treebank test data available"}
    
    if max_sentences:
        test_data = test_data[:max_sentences]
    
    print(f"\nüèÜ PENN TREEBANK EVALUATION")
    print(f"üìä Testing on {len(test_data)} sentences")
    
    correct = 0
    total = 0
    tag_stats = defaultdict(lambda: {"correct": 0, "total": 0})
    
    start_time = time.time()
    
    for i, (sentence, expected_tags) in enumerate(tqdm(test_data, desc="Evaluating")):
        try:
            # Get predictions
            predictions = predictor.predict(sentence)
            predicted_tags = [tag for word, tag in predictions]
            
            # Align predictions with expected (handle length mismatches)
            min_len = min(len(predicted_tags), len(expected_tags))
            
            for j in range(min_len):
                pred_tag = predicted_tags[j]
                true_tag = expected_tags[j]
                
                total += 1
                tag_stats[true_tag]["total"] += 1
                
                if pred_tag == true_tag:
                    correct += 1
                    tag_stats[true_tag]["correct"] += 1
        
        except Exception as e:
            print(f"Error processing sentence {i}: {e}")
            continue
    
    elapsed_time = time.time() - start_time
    
    # Calculate metrics
    accuracy = correct / total if total > 0 else 0
    
    # Per-tag accuracy
    tag_accuracies = {}
    for tag, stats in tag_stats.items():
        if stats["total"] > 0:
            tag_accuracies[tag] = stats["correct"] / stats["total"]
    
    results = {
        "accuracy": accuracy,
        "correct": correct,
        "total": total,
        "sentences": len(test_data),
        "time_seconds": elapsed_time,
        "tokens_per_second": total / elapsed_time if elapsed_time > 0 else 0,
        "tag_accuracies": tag_accuracies,
        "dataset": "Penn Treebank (NLTK sample)"
    }
    
    # Print results
    print(f"\nüìà PENN TREEBANK RESULTS:")
    print(f"Accuracy: {accuracy:.1%} ({correct:,}/{total:,} tokens)")
    print(f"Time: {elapsed_time:.2f}s ({results['tokens_per_second']:.0f} tokens/sec)")
    print(f"Sentences: {len(test_data):,}")
    
    # Show per-tag accuracy for common tags
    print(f"\nüè∑Ô∏è  PER-TAG ACCURACY:")
    common_tags = ['NOUN', 'VERB', 'ADJ', 'ADV', 'ADP', 'DET', 'PRON', 'PUNCT']
    for tag in common_tags:
        if tag in tag_accuracies and tag_stats[tag]["total"] >= 10:
            acc = tag_accuracies[tag]
            count = tag_stats[tag]["total"]
            print(f"{tag:8s}: {acc:.1%} ({count:,} tokens)")
    
    return results

def benchmark_penn_treebank_comparison(predictor, max_sentences=500):
    """
    Compare performance on Penn Treebank against NLTK and spaCy baselines.
    """
    print(f"\nü•ä PENN TREEBANK BENCHMARK COMPARISON")
    
    # Load test data
    test_data = load_penn_treebank_test()
    if not test_data:
        print("‚ùå No Penn Treebank data available for comparison")
        return {}
    
    if max_sentences:
        test_data = test_data[:max_sentences]
    
    print(f"üìä Comparing on {len(test_data)} sentences")
    
    results = {}
    
    # Test our model
    print("\nü§ñ Testing our model...")
    our_results = evaluate_on_penn_treebank(predictor, test_data)
    results['our_model'] = our_results
    
    # Test NLTK
    print("\nüìö Testing NLTK...")
    try:
        # Ensure NLTK data is available
        try:
            nltk.data.find('taggers/averaged_perceptron_tagger')
        except LookupError:
            print("Downloading NLTK POS tagger...")
            nltk.download('averaged_perceptron_tagger', quiet=True)
        
        start_time = time.time()
        correct = 0
        total = 0
        
        for sentence, expected_tags in tqdm(test_data, desc="NLTK"):
            tokens = predictor.tokenize(sentence)
            nltk_tagged = nltk.pos_tag(tokens)
            
            # Convert NLTK (Penn) tags to Universal
            universal_tagged = convert_penn_to_universal(nltk_tagged)
            predicted_tags = [tag for word, tag in universal_tagged]
            
            # Align and count
            min_len = min(len(predicted_tags), len(expected_tags))
            for j in range(min_len):
                total += 1
                if predicted_tags[j] == expected_tags[j]:
                    correct += 1
        
        elapsed_time = time.time() - start_time
        nltk_accuracy = correct / total if total > 0 else 0
        
        results['nltk'] = {
            "accuracy": nltk_accuracy,
            "correct": correct,
            "total": total,
            "time_seconds": elapsed_time,
            "tokens_per_second": total / elapsed_time if elapsed_time > 0 else 0
        }
        
    except Exception as e:
        print(f"‚ùå NLTK test failed: {e}")
        results['nltk'] = {"error": str(e)}
    
    # Test spaCy (optional)
    print("\nüöÄ Testing spaCy...")
    try:
        import spacy
        
        # Try to load spacy model
        try:
            nlp = spacy.load("en_core_web_sm")
        except OSError:
            print("üì¶ spaCy en_core_web_sm not found. Install with: python -m spacy download en_core_web_sm")
            results['spacy'] = {"error": "Model not available"}
        else:
            start_time = time.time()
            correct = 0
            total = 0
            
            for sentence, expected_tags in tqdm(test_data, desc="spaCy"):
                doc = nlp(sentence)
                predicted_tags = [token.pos_ for token in doc]
                
                # spaCy uses Universal POS tags by default
                min_len = min(len(predicted_tags), len(expected_tags))
                for j in range(min_len):
                    total += 1
                    if predicted_tags[j] == expected_tags[j]:
                        correct += 1
            
            elapsed_time = time.time() - start_time
            spacy_accuracy = correct / total if total > 0 else 0
            
            results['spacy'] = {
                "accuracy": spacy_accuracy,
                "correct": correct,
                "total": total,
                "time_seconds": elapsed_time,
                "tokens_per_second": total / elapsed_time if elapsed_time > 0 else 0
            }
    
    except ImportError:
        print("üì¶ spaCy not available. Install with: pip install spacy")
        results['spacy'] = {"error": "Not installed"}
    except Exception as e:
        print(f"‚ùå spaCy test failed: {e}")
        results['spacy'] = {"error": str(e)}
    
    # Print comparison table
    print(f"\nüìä PENN TREEBANK COMPARISON RESULTS:")
    print(f"{'Model':<12} {'Accuracy':<10} {'Speed (tok/s)':<12} {'Time (s)':<10}")
    print("-" * 50)
    
    for model_name, model_results in results.items():
        if 'error' not in model_results:
            acc = model_results['accuracy']
            speed = model_results['tokens_per_second']
            time_s = model_results['time_seconds']
            print(f"{model_name:<12} {acc:<10.1%} {speed:<12.0f} {time_s:<10.2f}")
        else:
            print(f"{model_name:<12} {'ERROR':<10} {'-':<12} {'-':<10}")
    
    return results

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", default="router_combined.pt", 
                        help="Path to saved model weights")
    parser.add_argument("--config", default=None,
                        help="Path to model configuration JSON file (auto-detected if not provided)")
    parser.add_argument("--text", type=str,
                        help="Text to analyze (if not provided, enters interactive mode)")
    parser.add_argument("--benchmark", action="store_true",
                        help="Compare with NLTK and spaCy baselines")
    parser.add_argument("--expected", type=str, nargs="*",
                        help="Expected POS tags for accuracy calculation (space-separated)")
    parser.add_argument("--batch", action="store_true",
                        help="Run large-scale batch testing on thousands of sentences")
    parser.add_argument("--batch-size", type=int, default=512,
                        help="Batch size for large-scale testing (default: 512)")
    parser.add_argument("--num-sentences", type=int, default=2000,
                        help="Number of sentences to test in batch mode (default: 2000)")
    parser.add_argument("--stress-test", action="store_true",
                        help="Run extreme scale stress test (100K+ sentences)")
    parser.add_argument("--max-batch-size", type=int, default=4096,
                        help="Maximum batch size for stress testing (default: 4096)")
    parser.add_argument("--optimize-batch-size", action="store_true",
                        help="Find and save optimal batch size for your GPU")
    parser.add_argument("--config-file", default="batch_config.json",
                        help="JSON file to store/load optimal batch size (default: batch_config.json)")
    parser.add_argument("--no-fine-tuning", action="store_true",
                        help="Skip fine-grained optimization (faster but less precise)")
    parser.add_argument("--penn-treebank", action="store_true",
                        help="Evaluate on Penn Treebank (NLTK sample)")
    parser.add_argument("--wsj-path", type=str, default=None,
                        help="Path to full Penn Treebank for WSJ sections 22-24 evaluation")
    parser.add_argument("--penn-benchmark", action="store_true",
                        help="Compare models on Penn Treebank")
    args = parser.parse_args()

    # Initialize predictor
    predictor = POSPredictor(args.model, args.config)

    # Penn Treebank evaluation mode
    if args.penn_treebank or args.wsj_path or args.penn_benchmark:
        print("üèÜ PENN TREEBANK EVALUATION MODE")
        
        if args.wsj_path:
            # Full WSJ sections 22-24 evaluation (requires complete Penn Treebank)
            try:
                test_data = load_wsj_sections_22_24(args.wsj_path)
                if test_data:
                    print("üéØ Evaluating on WSJ sections 22-24 (gold standard)")
                    results = evaluate_on_penn_treebank(predictor, test_data)
                    print("\nüìä This is the standard Penn Treebank WSJ test set!")
                else:
                    print("‚ùå Failed to load WSJ sections 22-24")
            except Exception as e:
                print(f"‚ùå Error loading WSJ data: {e}")
                print("üí° Make sure you have the complete Penn Treebank from LDC")
                
        elif args.penn_benchmark:
            # Compare models on Penn Treebank
            benchmark_penn_treebank_comparison(predictor, max_sentences=args.num_sentences)
            
        else:
            # NLTK Penn Treebank sample evaluation
            print("üìö Using NLTK Penn Treebank sample")
            print("üí° For full WSJ sections 22-24, use --wsj-path with complete Penn Treebank")
            results = evaluate_on_penn_treebank(predictor, max_sentences=args.num_sentences)
            
        return

    # Load optimal batch size if available
    optimal_config = load_optimal_batch_size(args.config_file)
    if optimal_config and (not hasattr(args, 'batch_size') or args.batch_size == 512):
        optimal_batch = optimal_config['optimal_batch_size']
        print(f"üí° Using previously optimized batch size: {optimal_batch:,}")
        print(f"   GPU: {optimal_config['gpu_info']}")
        print(f"   Peak throughput: {optimal_config['optimal_throughput']:.0f} tokens/sec")
        print(f"   Optimized on: {optimal_config['timestamp']}")
        args.batch_size = optimal_batch
    
    if args.optimize_batch_size:
        # Batch size optimization mode
        fine_tune = not args.no_fine_tuning
        config = predictor.optimize_batch_size(args.config_file, fine_tune=fine_tune)
        
    elif args.stress_test:
        # Extreme scale stress testing mode
        results = predictor.stress_test(args.max_batch_size)
        
    elif args.batch:
        # Large-scale batch testing mode
        print("üöÄ Large-Scale Batch Testing Mode")
        
        # Load test sentences
        test_sentences = load_test_sentences(args.num_sentences, predictor.config['vocabulary']['treebanks'][0])
        
        if args.benchmark:
            # Run full benchmark with all models
            results = predictor.benchmark_batch(test_sentences, args.batch_size)
            print_batch_results(results)
        else:
            # Test only our model
            print(f"\n‚ö° Testing our model on {len(test_sentences)} sentences...")
            start_time = time.time()
            predictions = predictor.predict_batch(test_sentences, args.batch_size)
            total_time = time.time() - start_time
            
            total_tokens = sum(len(predictor.tokenize(text)) for text in test_sentences)
            
            print(f"\nüìä BATCH PROCESSING RESULTS:")
            print(f"Sentences processed: {len(test_sentences)}")
            print(f"Total tokens: {total_tokens:,}")
            print(f"Total time: {total_time:.1f}s")
            print(f"Sentences/sec: {len(test_sentences) / total_time:.1f}")
            print(f"Tokens/sec: {total_tokens / total_time:.0f}")
            print(f"Average time per sentence: {(total_time / len(test_sentences)) * 1000:.1f}ms")
    
    elif args.text:
        if args.benchmark:
            # Parse expected tags if provided
            expected_tags = None
            if args.expected:
                tokens = predictor.tokenize(args.text)
                if len(args.expected) == len(tokens):
                    expected_tags = list(zip(tokens, args.expected))
                else:
                    print(f"Warning: Expected {len(tokens)} tags, got {len(args.expected)}")
            
            # Run benchmark comparison
            results = predictor.compare_with_baselines(args.text, expected_tags)
            print_comparison_results(results)
        else:
            # Simple prediction mode
            start_time = time.time()
            predictions = predictor.predict(args.text)
            inference_time = (time.time() - start_time) * 1000
            
            print(f"\nInput: {args.text}")
            print("POS predictions:")
            for token, pos in predictions:
                print(f"  {token:15} -> {pos}")
            print(f"\nInference time: {inference_time:.1f}ms")
            print(f"Speed: {len(predictions) / (inference_time / 1000):.0f} tokens/sec")
    else:
        # Interactive mode
        print("\n" + "="*60)
        print("Interactive POS Tagger")
        print(f"üìã Model: {predictor.config['model_name']} - {predictor.config['description']}")
        print("Enter text to analyze (or 'quit' to exit)")
        if args.benchmark:
            print("üèÜ Benchmark mode: comparing with NLTK and spaCy")
        print("üí° Tip: Use --batch for large-scale testing, --stress-test for extreme performance")
        print("üí° Tip: Use --penn-treebank for Penn Treebank evaluation")
        print("="*60)
        
        while True:
            try:
                text = input("\n> ").strip()
                if text.lower() in ['quit', 'exit', 'q']:
                    break
                if not text:
                    continue
                    
                if args.benchmark:
                    # Run benchmark comparison
                    results = predictor.compare_with_baselines(text)
                    print_comparison_results(results)
                else:
                    # Simple prediction mode
                    start_time = time.time()
                    predictions = predictor.predict(text)
                    inference_time = (time.time() - start_time) * 1000
                    
                    print("\nPOS predictions:")
                    for token, pos in predictions:
                        print(f"  {token:15} -> {pos}")
                    print(f"\nInference time: {inference_time:.1f}ms")
                    print(f"Speed: {len(predictions) / (inference_time / 1000):.0f} tokens/sec")
                    
            except KeyboardInterrupt:
                break
        
        print("\nGoodbye!")

if __name__ == "__main__":
    main() 